# TensorFlow-Advanced-Techniques-Specialization
<h3>
Course 1: Custom Models, Layers, and Loss Functions with TensorFlow
  </h3>
  <h4>
This is the first course of the TensorFlow: Advanced Techniques Specialization.<br><br>
Week 1: Functional API<br>
Compare Functional and Sequential APIs, discover new models you can build with the Functional API, and build a model that produces multiple outputs including a Siamese network.<br><br>
Week 2: Custom Loss Functions<br>
Build custom loss functions (including the contrastive loss function used in a Siamese network) in order to measure how well a model is doing and help your neural network learn from training data.<br><br>
Week 3: Custom Layers<br><br>
Build off of existing standard layers to create custom layers for your models, customize a network layer with a lambda layer, understand the differences between them, learn what makes up a custom layer, and explore activation functions.<br><br>
Week 4: Custom Models<br><br>
Build off of existing models to add custom functionality, learn how to define your own custom class instead of using the Functional or Sequential APIs, build models that can be inherited from the TensorFlow Model class, and build a residual network (ResNet) through defining a custom model class.<br><br>
Bonus: Callbacks<br><br>
Customize your model outputs and its behavior during training through custom callbacks, implement a custom callback to stop training once the callback detects overfitting, use a model checkpoint to save parameters during training, use the EarlyStopping callback to keep a model from overfitting, and become familiar with where you might want to create a custom callback.<br><br>
  </h4>
  <br>
 <h3>
Course 2: Custom and Distributed Training with TensorFlow
  </h3>
  <h4>
This is the second course of the TensorFlow: Advanced Techniques Specialization.
  Week 1: Differentiation and Gradients
Learn about Tensor objects, the fundamental building blocks of TensorFlow, understand the difference between the eager and graph modes in TensorFlow, and learn how to use a TensorFlow tool to calculate gradients.
Week 2: Custom Training
Build your own custom training loops using GradientTape and TensorFlow Datasets to gain more flexibility and visibility with your model training.
Week 3: Graph Mode
Learn about the benefits of generating code that runs in graph mode, take a peek at what graph code looks like, and practice generating this more efficient code automatically with TensorFlowâ€™s tools.
Week 4: Distributed Training
Harness the power of distributed training to process more data and train larger models, faster, get an overview of various distributed training strategies, and practice working with a strategy that trains on multiple GPU cores, and another that trains on multiple TPU cores.
  </h4>
